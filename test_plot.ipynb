{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.metrics import log_loss\n",
    "from decentralized_SGD_logistic import DecentralizedSGDLogistic\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from scipy.special import expit as sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RCV dataset from ChocoSGD Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logistic(A, y, param):\n",
    "    m = DecentralizedSGDLogistic(param)\n",
    "    res = m.fit(A, y)\n",
    "    print('{} - score: {}'.format(param, m.score(A, y)))\n",
    "    return res, m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    }
   ],
   "source": [
    "dataset_path = os.path.expanduser('../ChocoSGD/data/rcv1_test.binary.bz2')\n",
    "print('Loading dataset...')\n",
    "A, y = load_svmlight_file(dataset_path)\n",
    "A_p = A[:100000]\n",
    "y_p = y[:100000]\n",
    "y_p = 1*(y_p > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the baseline with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adirlou/.local/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97885\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier(loss='log', penalty='l2',alpha =1/A_p.shape[0])\n",
    "clf.fit(A_p, y_p)\n",
    "print(clf.score(A_p, y_p))\n",
    "x_predict = clf.predict(A_p)\n",
    "exact_optimum = log_loss(x_predict, y_p) + (1 / A_p.shape[0]) * np.sum(x_predict**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute with Decentralized SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = 1 / A_p.shape[0]\n",
    "n_features = A_p.shape[1]\n",
    "\n",
    "params_disconnected = Parameters(num_epoch=5, lr_type='bottou',\n",
    "                           initial_lr=10, tau=n_features, regularizer=reg,\n",
    "                           quantization='full', n_machines=128,\n",
    "                           method='plain',topology='complete', estimate='final',\n",
    "                           split_data_random_seed=2, distribute_data=True,\n",
    "                           split_data_strategy='naive')\n",
    "\n",
    "res_disconnected, dec_log = run_logistic(A_p, y_p, params_disconnected)\n",
    "\n",
    "\"\"\"params_ring = dict(name=\"chocosgd-centralized\", num_epoch=5, lr_type='bottou',\n",
    "                           initial_lr=0.2, tau=n_features, regularizer=reg,\n",
    "                           quantization='full', n_cores=10,\n",
    "                           method='plain', topology='centralized', estimate='final',\n",
    "                           split_data_random_seed=2, distribute_data=True,\n",
    "                           split_data_strategy='naive')\n",
    "\n",
    "res_centralized = run_logistic(A_p, y_p, params_ring)\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higgs Boson Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logistic(A, y, param):\n",
    "    m = DecentralizedSGDLogistic(**param)\n",
    "    list_losses = m.fit(A, y)\n",
    "    print('{} - score: {}'.format(param, m.score(A, y)))\n",
    "    return list_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data(data_path):\n",
    "    \"\"\"Loads data and returns y (class labels), tX (features) and ids (event ids)\"\"\"\n",
    "    y = np.genfromtxt(data_path, delimiter=\",\", skip_header=1, dtype=str, usecols=1)\n",
    "    x = np.genfromtxt(data_path, delimiter=\",\", skip_header=1)\n",
    "    input_data = x[:, 2:]\n",
    "\n",
    "    # convert class labels from strings to binary (-1,1)\n",
    "    yb = np.ones(len(y))\n",
    "    yb[np.where(y=='b')] = -1\n",
    "\n",
    "    return yb, input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(input_data, mean=False):\n",
    "\n",
    "    #Replace -999 by most frequent value of column\n",
    "    for i in range(input_data.shape[1]):\n",
    "        current_col = input_data[:, i]\n",
    "\n",
    "        if -999.0 in current_col:\n",
    "            indices_to_change = (current_col == -999.0)\n",
    "            if mean:\n",
    "                curr_mean = np.mean(current_col[~indices_to_change])\n",
    "                current_col[indices_to_change] = curr_mean\n",
    "            else:\n",
    "                (values,counts) = np.unique(current_col[~indices_to_change], return_counts=True)\n",
    "                ind=np.argmax(counts)\n",
    "                current_col[indices_to_change] = values[ind] if len(values) > 0 else 0\n",
    "\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    \"\"\"Standardize the given data\"\"\"\n",
    "    means = x.mean(0)\n",
    "    stds = x.std(0)\n",
    "    return (x - means)/stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, A = load_csv_data('train.csv')\n",
    "A = standardize(clean(A, True))\n",
    "y = 1 *(y > 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the baseline with SGD classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.744164\n",
      "Final loss: 0.5433024027306941\n"
     ]
    }
   ],
   "source": [
    "# Fit a SGD\n",
    "clf = SGDClassifier(loss='log', penalty='l2',alpha =1/A.shape[0], tol=1e-4, n_jobs=1)\n",
    "clf.fit(A, y)\n",
    "\n",
    "accuracy = clf.score(A, y)\n",
    "optimum_x = clf.predict_proba(A)[:, 1]\n",
    "\n",
    "# Optimal loss, useful for plots\n",
    "optimum_loss = (-(y.T.dot(np.log(optimum_x)) + (1 - y).T.dot(np.log(1 - optimum_x))) / A.shape[0])\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Final loss:', optimum_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing with Decentralized SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of indices: 512\n",
      "length of last machine indices: 488\n",
      "current iteration: 0 epoch: 0 epoch_iteration 0 loss 1.6532019533113749 elapsed 0.023987531661987305s\n",
      "current iteration: 9 epoch: 0 epoch_iteration 9 loss 1.432588552377515 elapsed 0.26162004470825195s\n",
      "current iteration: 18 epoch: 0 epoch_iteration 18 loss 1.2669745859491743 elapsed 0.48531174659729004s\n",
      "current iteration: 27 epoch: 0 epoch_iteration 27 loss 1.1404796781369295 elapsed 0.7504069805145264s\n",
      "current iteration: 36 epoch: 0 epoch_iteration 36 loss 1.0444224931943078 elapsed 0.9808566570281982s\n",
      "current iteration: 45 epoch: 0 epoch_iteration 45 loss 0.965608208770154 elapsed 1.231112003326416s\n",
      "current iteration: 54 epoch: 0 epoch_iteration 54 loss 0.9024571572583773 elapsed 1.4517929553985596s\n",
      "current iteration: 63 epoch: 0 epoch_iteration 63 loss 0.8569154764402902 elapsed 1.6804826259613037s\n",
      "current iteration: 72 epoch: 0 epoch_iteration 72 loss 0.8216190906627076 elapsed 1.9096367359161377s\n",
      "current iteration: 81 epoch: 0 epoch_iteration 81 loss 0.7914523087722962 elapsed 2.1598901748657227s\n",
      "current iteration: 90 epoch: 0 epoch_iteration 90 loss 0.766080923699576 elapsed 2.4165496826171875s\n",
      "current iteration: 99 epoch: 0 epoch_iteration 99 loss 0.7462171771409185 elapsed 2.6366913318634033s\n",
      "current iteration: 108 epoch: 0 epoch_iteration 108 loss 0.7277401121648369 elapsed 2.864755153656006s\n",
      "current iteration: 117 epoch: 0 epoch_iteration 117 loss 0.7151533907984461 elapsed 3.0965616703033447s\n",
      "current iteration: 126 epoch: 0 epoch_iteration 126 loss 0.7018337195933751 elapsed 3.367551326751709s\n",
      "current iteration: 135 epoch: 0 epoch_iteration 135 loss 0.689236544933553 elapsed 3.6282074451446533s\n",
      "current iteration: 144 epoch: 0 epoch_iteration 144 loss 0.679434377884666 elapsed 3.8568172454833984s\n",
      "current iteration: 153 epoch: 0 epoch_iteration 153 loss 0.6696025063384605 elapsed 4.084038496017456s\n",
      "current iteration: 162 epoch: 0 epoch_iteration 162 loss 0.6627280239668754 elapsed 4.341521263122559s\n",
      "current iteration: 171 epoch: 0 epoch_iteration 171 loss 0.6540893125800356 elapsed 4.573228120803833s\n",
      "current iteration: 180 epoch: 0 epoch_iteration 180 loss 0.6466771228796467 elapsed 4.8194286823272705s\n",
      "current iteration: 189 epoch: 0 epoch_iteration 189 loss 0.6408047510724487 elapsed 5.052741765975952s\n",
      "current iteration: 198 epoch: 0 epoch_iteration 198 loss 0.6355852305510081 elapsed 5.307607173919678s\n",
      "current iteration: 207 epoch: 0 epoch_iteration 207 loss 0.630575945971484 elapsed 5.54088282585144s\n",
      "current iteration: 216 epoch: 0 epoch_iteration 216 loss 0.6238812547772156 elapsed 5.773991346359253s\n",
      "current iteration: 225 epoch: 0 epoch_iteration 225 loss 0.6204538619197418 elapsed 6.007954359054565s\n",
      "current iteration: 234 epoch: 0 epoch_iteration 234 loss 0.6175062433896942 elapsed 6.260524034500122s\n",
      "current iteration: 243 epoch: 0 epoch_iteration 243 loss 0.6136508157679978 elapsed 6.490005731582642s\n",
      "current iteration: 252 epoch: 0 epoch_iteration 252 loss 0.6105695351322694 elapsed 6.723560571670532s\n",
      "current iteration: 261 epoch: 0 epoch_iteration 261 loss 0.6086852913261559 elapsed 6.956562757492065s\n",
      "current iteration: 270 epoch: 0 epoch_iteration 270 loss 0.605777229309236 elapsed 7.2126219272613525s\n",
      "current iteration: 279 epoch: 0 epoch_iteration 279 loss 0.603324021455742 elapsed 7.441030979156494s\n",
      "current iteration: 288 epoch: 0 epoch_iteration 288 loss 0.600319502068321 elapsed 7.674180030822754s\n",
      "current iteration: 297 epoch: 0 epoch_iteration 297 loss 0.5988930384141637 elapsed 7.909075975418091s\n",
      "current iteration: 306 epoch: 0 epoch_iteration 306 loss 0.5964356006367034 elapsed 8.195017576217651s\n",
      "current iteration: 315 epoch: 0 epoch_iteration 315 loss 0.595222753139927 elapsed 8.427730560302734s\n",
      "current iteration: 324 epoch: 0 epoch_iteration 324 loss 0.5940055936997701 elapsed 8.655529022216797s\n",
      "current iteration: 333 epoch: 0 epoch_iteration 333 loss 0.5930385138226512 elapsed 8.885616540908813s\n",
      "current iteration: 342 epoch: 0 epoch_iteration 342 loss 0.5910287055346021 elapsed 9.124172449111938s\n",
      "current iteration: 351 epoch: 0 epoch_iteration 351 loss 0.590673318147005 elapsed 9.443333864212036s\n",
      "current iteration: 360 epoch: 0 epoch_iteration 360 loss 0.5896157236383115 elapsed 9.678512573242188s\n",
      "current iteration: 369 epoch: 0 epoch_iteration 369 loss 0.5888498142071257 elapsed 9.910439252853394s\n",
      "current iteration: 378 epoch: 0 epoch_iteration 378 loss 0.5871983815091902 elapsed 10.150110483169556s\n",
      "current iteration: 387 epoch: 0 epoch_iteration 387 loss 0.5860954182922103 elapsed 10.39451265335083s\n",
      "current iteration: 396 epoch: 0 epoch_iteration 396 loss 0.5849817605284082 elapsed 10.632101058959961s\n",
      "current iteration: 405 epoch: 0 epoch_iteration 405 loss 0.5835089331606191 elapsed 10.87288761138916s\n",
      "current iteration: 414 epoch: 0 epoch_iteration 414 loss 0.5837303760463252 elapsed 11.109256029129028s\n",
      "current iteration: 423 epoch: 0 epoch_iteration 423 loss 0.58191285807498 elapsed 11.36996054649353s\n",
      "current iteration: 432 epoch: 0 epoch_iteration 432 loss 0.5812448102963841 elapsed 11.604905843734741s\n",
      "current iteration: 441 epoch: 0 epoch_iteration 441 loss 0.5814869328221701 elapsed 11.843299865722656s\n",
      "current iteration: 450 epoch: 0 epoch_iteration 450 loss 0.5804986290904316 elapsed 12.079966306686401s\n",
      "current iteration: 459 epoch: 0 epoch_iteration 459 loss 0.5795097325642863 elapsed 12.371693849563599s\n",
      "current iteration: 468 epoch: 0 epoch_iteration 468 loss 0.5790623790370798 elapsed 12.662119150161743s\n",
      "current iteration: 477 epoch: 0 epoch_iteration 477 loss 0.5782546774250596 elapsed 12.903384685516357s\n",
      "current iteration: 486 epoch: 0 epoch_iteration 486 loss 0.5782047709975288 elapsed 13.164991617202759s\n",
      "epoch 0: loss 0.5780538757690913 score 0.723332\n",
      "Training took: 13.20303750038147s\n",
      "{'num_epoch': 1, 'lr_type': 'bottou', 'initial_lr': 0.1, 'tau': 30, 'regularizer': 4e-06, 'quantization': 'full', 'n_machines': 512, 'method': 'plain', 'topology': 'complete', 'estimate': 'final', 'split_data_random_seed': 2, 'distribute_data': True, 'split_data_strategy': 'naive'} - score: 0.723332\n"
     ]
    }
   ],
   "source": [
    "reg = 1 / A.shape[0]\n",
    "n_features = A.shape[1]\n",
    "\n",
    "params_disconnected = dict(num_epoch=1, lr_type='bottou',\n",
    "                           initial_lr=0.1, tau=n_features, regularizer=reg,\n",
    "                           quantization='full', n_machines=512,\n",
    "                           method='plain',topology='complete', estimate='final',\n",
    "                           split_data_random_seed=2, distribute_data=True,\n",
    "                           split_data_strategy='naive')\n",
    "\n",
    "list_losses = run_logistic(A, y, params_disconnected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
